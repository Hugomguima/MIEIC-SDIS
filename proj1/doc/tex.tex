	\documentclass[11pt]{article}

\usepackage[english]{babel}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage[section]{placeins}
\usepackage{amssymb} % for \smallsetminus

\usepackage{lmodern}  % for bold teletype font
\usepackage{amsmath}  % for \hookrightarrow
\usepackage{xcolor}   % for \textcolor
\usepackage{listings}

\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}

\lstdefinestyle{CStyle}{
    backgroundcolor=\color{backgroundColour},   
    commentstyle=\color{mGreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{mGray},
    stringstyle=\color{mPurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    language=C
}

\definecolor{javared}{rgb}{0.6,0,0} % for strings
\definecolor{javagreen}{rgb}{0.25,0.5,0.35} % comments
\definecolor{javapurple}{rgb}{0.5,0,0.35} % keywords
\definecolor{javadocblue}{rgb}{0.25,0.35,0.75} % javadoc
 
\lstset{language=Java,
basicstyle=\ttfamily,
keywordstyle=\color{javapurple}\bfseries,
stringstyle=\color{javared},
commentstyle=\color{javagreen},
morecomment=[s][\color{javadocblue}]{/**}{*/},
numbers=left,
numberstyle=\tiny\color{black},
stepnumber=1,
numbersep=10pt,
tabsize=4,
showspaces=false,
showstringspaces=false}

\begin{document}

\begin{titlepage}
	\begin{center}
		\vspace*{1cm}
		
		\Large
		\textbf{Distributed Backup Service}
		
		\vspace{0.5cm}
		\large
		Project 1 
		
		\vspace{1.5cm}
		
		\textbf{Hugo Miguel Monteiro Guimarães}\\
		\textbf{Paulo Jorge Salgado Marinho Ribeiro}
		
		\vspace{4cm}
		
		Work carried out within the scope of \\
		Course Unit Distributed Systems
		
		\vspace{0.8cm}
		
		\includegraphics[width=0.4 \textwidth]{feup_logo.png}
		
		\vspace{1.5cm}		
		
		\large
		Master in Informatics and Computing Engineering\\
		Faculdade de Engenharia da Universidade\\
		do Porto\\
		9th march 2021
	
	\end{center}
\end{titlepage}


\pagebreak
\tableofcontents

\pagebreak

\section*{Introduction}
This report is dividided into two sections

\begin{itemize}
	\item Enhancements - Explanation of the implemented protocol enhancements
	\begin{itemize}
		\item Backup Enhancement  
		\item Restore Enhancement 
		\item Delete Enhancement 
	\end{itemize}
	\item Concurrency - Explanation of Concurrency implementation in our project
\end{itemize}

\section{Concurrency Design}

This section analyzes our implementation of the concurrent execution of instances of the required protocols.
Both versions of the developed program allow simultaneous executions of these protocols.

When a peer starts running, the class PeerInitializer begins by storing the input parameters.


The peer information is organized in the class DataStored, which contains several thread-safe ConcurrentHashMaps and CopyOnWriteArraySets. The information inside this class is maintained through multiple program executions, since we serialize its data on program shutdown, saving it in a file. This data is then read as soon as the peer starts running.

On invocation, the class Peer will also be responsible for initializing the three multicast channels:
\begin{itemize}
	\item MulticastControlChannel, used for control messages.
	\item MulticastDataChannel, used for backing up file chunk data.
 	\item MulticastDataRecovery, used for restoring file chunk data.
\end{itemize}

\begin{lstlisting}[language=java]
// Initialize mc channel
controlChannel = new MulticastControlChannel(mcAddr, mcPort, peerID);
// Initialize mdb channel
backupChannel = new MulticastDataChannel(mdrAddr, mdrPort, peerID);
// Initialize mdr channel
restoreChannel = new MulticastDataRecovery(mdbAddr, mdbPort, peerID);

executor.execute(controlChannel);
executor.execute(backupChannel);
executor.execute(restoreChannel);
\end{lstlisting}


Each of these classes extends the class MulticastChannel, which implements the Java Runnable Interface, to execute code on a concurrent thread.
This interface requires the implementation of the method run(), with void as return type, by the instances of that class.
This way, each channel is executed as a single thread, by an object of class ScheduledThreadPoolExecutor with a core pool size of 250 threads, to avoid the invocation overhead of creating a new thread for each execution needed.
When they are executed, the MulticastChannel run() method will be called, which consists of an infinite while loop, whose function is to receive data packets corresponding to the messages to be interchanged by the peers.

Whenever a message is received, the peer's ScheduledThreadPoolExecutor starts a new thread of the class MessageHandler, which also implements the interface Runnable.

In this class, the message received will be parsed by the class MessageParser, in other words, the message components will be divided, to be used in the future.
One of those components is the sender ID, which is compared to the ID of the peer that received the message, to ignore self-messages.

Another component is the message type, used to decide what protocol will be executed. Our program has protocols defined for the messages of types PUTCHUNK, STORED, GETCHUNK, CHUNK, DELETE, REMOVED, DELETED, and HELLO (the last two are only executed on the enhanced version, 2.0).
Despite this, every other type is accepted but ignored, for interoperability purposes.

// meter print do run() do MulticastChannel

\begin{lstlisting}[language=java]
@Override
public void run() {
	byte[] msgReceived = new byte[65507]; // maximum data size for UDP packet -> https://en.wikipedia.org/wiki/User_Datagram_Protocol

    try {
        while(true) {
            // Waiting to receive a packet
            DatagramPacket requestPacket = new DatagramPacket(msgReceived, msgReceived.length);
            this.multicastSocket.receive(requestPacket);
            byte[] realData = Arrays.copyOf(requestPacket.getData(), requestPacket.getLength());
            Peer.executor.execute(new MessageHandler(realData, this.peerID, requestPacket.getAddress()));
        }
    } catch(Exception e) {
        e.printStackTrace();
        System.out.println("Error receiving message from Multicast Data Channel (MDB)");
    }
}
\end{lstlisting}

Finally, the majority of the functions responsible for handling each message type launch a new thread that will execute the requested protocol.
This is done using the method schedule of the ScheduledThreadPoolExecutor, which initializes the desired thread only after the given delay.
We followed the project's guide advice of using a random delay uniformly distributed between 0 and 400 milliseconds.
That way, we lower the probability of the protocol being executed at the same time by different peers, avoiding "collisions" and assuring that the many peers that received the same message start and finish the required protocol at different times.
One example of this is the launching of the thread ChunkThread, while handling a GETCHUNK message, which will be responsible for sending the data of the chunk requested if the peer that received the message has a local copy of that chunk.


A different thread is also launched every time we need to send a message to the multicast channels, which will execute the function sendMessage() of the MulticastChannel class, after a random delay.

\begin{lstlisting}[language=java]
void sendMessage(byte[] message) {
    try {
        DatagramPacket packetSend = new DatagramPacket(message, message.length, this.addr, this.port);
        this.multicastSocket.send(packetSend);
    } catch(Exception e) {
        e.printStackTrace();
        System.out.println("Error sending message to Multicast Data Channel (MDB)");
    }
}
\end{lstlisting}

On the client-side, the TestApp class starts by parsing the command line arguments and requesting the desired protocol.
This is done by calling the corresponding function of the PeerProtocol object of the peer with the RMI object name given as input to the TestApp.
After this, the function will start a new thread, responsible for executing that protocol, in the vision of the initiator peer.


Finally, during some protocols, some new threads are raised to execute some minor tasks at the same time that the main protocol is occurring.
The class PutChunkThread is an example of these calls, since it happens during the backup protocol, and for each chunk created during this protocol. Its purpose is to send the PUTCHUNK messages of each chunk created.
So, as the project guide suggests, the initiator will send at most 5 PUTCHUNK messages per chunk, in the case of not receiving the desired confirmation messages (replication degree), and doubles the delay between each resend.
In short, this mechanism is done by this class PutChunkThread, using an independent thread for each chunk to backup.


// ESCREVER PARTE DA GetChunkThread SÓ DEPOIS DE CORRIGIR O CÓDIGO DA VERSÃO 1.0

\pagebreak

\section{Enhancements}

We have implemented all required enhancements, which will be described in the following subsections.

In order to run the enhanced version of the project, each peer must be initialized with the version argument 2.0.

Example of initialization of one peer:
\begin{lstlisting}[language=java]
..\scripts\peer.sh 2.0 1 Peer1 225.0.0.1 8000 225.0.0.1 8001 225.0.0.1 8002
\end{lstlisting}

\bigskip
\pagebreak
\subsection{Backup Enhancement}
Since the 1.0 version of the Backup protocol only verifies the replication degree before sending a PUTCHUNK message via UDP Multicast, all the peers that receive that message will store the chunk.

This scheme is problematic, since it depletes the the backup space rather quickly by storing unnecessary addicional chunks.

We have solved this problem using a simple and efficient method.

Since all peers are aware of the replication degree of every chunk, via our chunkRepDegrees ConcurrentHashMap, which is updated in the STORED message handler, we are able to verify if a given chunk is already being stored
by another Peer. Hence, we have fixed this issue by prohibiting the storage of a new chunk if the replication degree has already been fulfilled.

\begin{lstlisting}[language=java]
// Backup enhancement
if(Peer.getVersion().equals("2.0")) {
	int chunkRepDeg = getChunkReplicationNum(chunkID);
	int desiredRepDeg = chunk.getDesiredReplicationDegree();
	if (chunkRepDeg >= desiredRepDeg) {
		System.out.println("Chunk " + chunkID + " already fulfilled repDegree. Ignoring chunk...");
		return;
	}
}
\end{lstlisting}

Although this method is simple, it is not perfect.
Given there are many concurrent threads running attempting to backup the chunk, there are read and write operation constantly being issued to the hashMap.
Even though the data structure is thread-safe, there is no guarantee that the number of chunks stored will be exactly the same as the desired replication degree everytime.

There is a possible way to fix this problem, by resending the protocol until the replication degree value is exactly the same as the desired, deleting exceeding chunks.

Although the aforementioned procedure would present as a solution, we have decided to not implement it, 
as it would flood the multicast channels with several other messages without significantly reducing the space depletion problem.

\pagebreak

\subsection{Restore Enhancement}

This enhancement was designed to avoid sending all the file's chunks through a UDP Multicast channel. As a matter of fact, the 1.0 version is incapable of restoring large files,
given that the UDP protocol is unreliable and there is too much data being continuously sent through the multicast channel. Many messages were often missing, and most of them would be ignored by every peer except the initiator,
which is the only one that needs the chunk data to restore the file.

In order to solve this problem, we came up with a solution by using the TCP protocol to exchange the body of the message between the peer that contains the chunk and the initiator peer requesting the protocol.

First, we initialized the TCPHandler thread, which is blocked in a while loop, launching a ClientHandler thread whenever there is a Client Socket starting the TCP connection.

\begin{lstlisting}[language=java]
@Override
public void run() {
	while(true) {
		try {
			new ClientHandler(serverSocket.accept()).start();
		} catch (IOException e) {
			e.printStackTrace();
		}
	}
}
\end{lstlisting}

The ClientHandler thread reads a CHUNK message through the estabilished TCP connection, and restores the file as soon as all the chunks have been received, by launching a GetChunkThread.

The establishment of the TCP connection is made for each chunk after a Peer that has a local copy of that chunk receives a GETCHUNK message and initializes a ClientSocket.

In order to implement this enhancement, we have changed the header of the GETCHUNK subprotocol by adding an extra line containing the ServerSocket port of the initiator Peer.
This port is essencial to estabilish the TCP connection, as well as the ipAdrress used for the server socket, which is not sent in the message header, given that it can already be accessed
via the UDP Multicast DatagramPacket getAddress() method.

Even though the body of the message is now being sent through a TCP connection to the initiator peer, the multicast channel is still used to send the header of the 
CHUNK message. Such is necessary because all peers must update their storage to be able to know if the chunk has already been sent by another peer, preventing replicated CHUNK messages.

\pagebreak

\subsection{Delete Enhancement}

In the 1.0 version of our project, if a peer that backs up some chunks of the file is not running at the time the initiator peer sends a DELETE message for that file, the space used by these chunks will never be reclaimed.

This enhancement aims to fix that problem, by creating two new messages, DELETED and HELLO.

The fist one has this structure:
\begin{lstlisting}[language=java]
   <Version> DELETED <SenderId> <FileId> <CRLF><CRLF>
\end{lstlisting}

This message works as a confirmation of the DELETE message, being sent after a peer successfully removes the chunks of the file requested by the DELETE message.

This confirmation message allows us to update our deletedFiles CopyOnWriteArraySet that contains the IDs of the files that have not yet been fully deleted by all other peers.

We have also created the HELLO message, whose structure is:
\begin{lstlisting}[language=java]
   <Version> HELLO <SenderId> <CRLF><CRLF>
\end{lstlisting}


which is sent by every Peer via Multicast as soon has it is starts running. Subsequently, all peers receiving this message will check the deletedFiles CopyOnWriteArraySet and initialize the DELETE subprotocol for every file that is yet to be fully deleted.

This way,we have managed to remove the chunks that couldnt't be fully deleted because a peer with a local copy of it was not running


\end{document}