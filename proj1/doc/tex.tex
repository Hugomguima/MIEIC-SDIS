\documentclass[11pt]{article}

\usepackage[english]{babel}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage[section]{placeins}
\usepackage{amssymb} % for \smallsetminus

\usepackage{lmodern}  % for bold teletype font
\usepackage{amsmath}  % for \hookrightarrow
\usepackage{xcolor}   % for \textcolor
\usepackage{listings}

\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}

\lstdefinestyle{CStyle}{
    backgroundcolor=\color{backgroundColour},   
    commentstyle=\color{mGreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{mGray},
    stringstyle=\color{mPurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    language=C
}

\definecolor{javared}{rgb}{0.6,0,0} % for strings
\definecolor{javagreen}{rgb}{0.25,0.5,0.35} % comments
\definecolor{javapurple}{rgb}{0.5,0,0.35} % keywords
\definecolor{javadocblue}{rgb}{0.25,0.35,0.75} % javadoc
 
\lstset{language=Java,
basicstyle=\ttfamily,
keywordstyle=\color{javapurple}\bfseries,
stringstyle=\color{javared},
commentstyle=\color{javagreen},
morecomment=[s][\color{javadocblue}]{/**}{*/},
numbers=left,
numberstyle=\tiny\color{black},
stepnumber=1,
numbersep=10pt,
tabsize=4,
showspaces=false,
showstringspaces=false}

\begin{document}

\begin{titlepage}
	\begin{center}
		\vspace*{1cm}
		
		\Large
		\textbf{Distributed Backup Service}
		
		\vspace{0.5cm}
		\large
		Project 1 
		
		\vspace{1.5cm}
		
		\textbf{Hugo Miguel Monteiro Guimarães}\\
		\textbf{Paulo Jorge Salgado Marinho Ribeiro}
		
		\vspace{4cm}
		
		Work carried out within the scope of \\
		Course Unit Distributed Systems
		
		\vspace{0.8cm}
		
		\includegraphics[width=0.4 \textwidth]{feup_logo.png}
		
		\vspace{1.5cm}		
		
		\large
		Master in Informatics and Computing Engineering\\
		Faculdade de Engenharia da Universidade\\
		do Porto\\
		9th march 2021
	
	\end{center}
\end{titlepage}


\pagebreak
\tableofcontents

\pagebreak

\section*{Introduction}
This report is dividided into two sections

\begin{itemize}
	\item Enhancements - Explanation of the implemented protocol enhancements
	\begin{itemize}
		\item Backup Enhancement  
		\item Restore Enhancement 
		\item Delete Enhancement 
	\end{itemize}
	\item Concurrency - Explanation of Concurrency implementation in our project
\end{itemize}

\section{Concurrency Design}

This section analyzes our implementation of the concurrent execution of instances of the required protocols.
Both versions of the developed program allow simultaneous executions of these protocols, requested by different initiator peers.

When a peer is initialized, the class PeerInitializer starts by storing the input parameters, including the multicast channels IP adresses and ports.
Then, it invokes the class Peer, which will be responsible for initializing the three multicast channels:
 - MulticastControlChannel, used for control messages.
 - MulticastDataChannel, used for backing up file chunk data.
 - MulticastDataRecovery, used for restoring file chunk data.

// meter print das chamadas do executor e criação

Each of these classes extends the class MulticastChannel, which implements the Java Runnable Interface, in order to execute code on a concurrent thread.
This interface requires the implementation of the method run(), with void as return type, by the instances of that class.
This way, each channel is executed as a single thread, by an object of class ScheduledThreadPoolExecutor with a core pool size of 250 threads, to avoid creating a new thread for each execution needed, since creating and terminating threads has some overhead.
When they are executed, the MulticastChannel run() method will be called, which consists of an infinite while loop, whose function is to receive data packets corresponding to the messages to be interchanged by the peers.

Whenever a message is received, the peer's ScheduledThreadPoolExecutor, which we will designate "executor" in this report, starts a new thread of the class MessageHandler, that also implements the interface Runnable.
In this class, the message received will be parsed by the class MessageParser, in other words the message components will be divided, to be used in the future.
One of those components is the sender ID, which is compared to the ID of the peer that received the message, in order to ignore self-messages.
Another component is the message type, used to decide what protocol will be executed. Our program has protocols defined for the messages of types PUTCHUNK, STORED, GETCHUNK, CHUNK, DELETE, REMOVED, DELETED and HELLO (some of this only executed on enhanced version, 2.0).
Despite this, every other type is accepted but ignored, for interoperability purposes.

// meter print do run() do MulticastChannel

Finally, the majority of the functions responsible for handling each message type launches a new thread which will execute the requested protocol.
This is done using the method schedule of the ScheduledThreadPoolExecutor, which initializes the desired thread only after the given delay.
We followed the project's enunciate advice of using a random delay uniformly distributed between 0 and 400 milliseconds.
That way, we lower the probability of the protocol being executed at the exact same time by different peers, avoiding "collisions" and assuring that the many peers that received the same message start and finish the required protocol at different times.

// meter print de algumas funções do Messagehandler que façam isto

A different thread is also launched every time we need to send a message to the multicast channels, which will execute the function sendMessage() of the MulticastChannel class, after a random delay.

// meter print da função sendMessage()

On the other side, the TestApp class starts by parsing the command line arguments and requesting the desired protocol.
This is done by calling the corresponding function of the PeerProtocol object of the peer with the RMI object name given as input to the TestApp.
After this, the function will start a new thread, responsible for executing that protocol, in the vision of the initiator peer.

// meter prints dos métodos da PeerProtocol

Finally, during some protocols, some new threads are raised with the objective of executing some minor tasks at the same time that the main protocol is ocurring.
The class PutChunkThread is an example of these calls, since it happens during the backup protocol, and for each 

// acabar

\section{Enhancements}

We have implemented all required enhancements, which will be described in the following subsections.

In order to run the project with the implemented enhancements, each peer must be initialized with the version argument 2.0

Example of initialization of one peer:
\[..\setminus scripts\setminus peer.sh 2.0 1 Peer1 225.0.0.1 8000 225.0.0.1 8001 225.0.0.1 8002\]

\subsection{Backup Enhancement}
Since the 1.0 version of the Backup protocol only verifies the replication degree before sending a PUTCHUNK message via UDP Multicast, if the chunk is sent correctly,
all remaining peers will store the chunk, as long as the replication degree value is fullfilled.

This scheme is problematic, since it depletes the the backup space rather quickly by storing unnecessary addicional chunks.

We have solved this problem using a simple and efficient method.

Since all Peers are aware of the replication degree of every chunk, via our chunkRepDegrees ConcurrentHashMap, we are able to verify if a given chunk is already being stored
by another Peer. Hence we have fixed this issue by prohibiting the storage of a new chunk if another peer has already been responsible for saving it.

\begin{lstlisting}[language=java]
// Backup enhancement
if(Peer.getVersion().equals("2.0")) {
	int chunkRepDeg = getChunkReplicationNum(chunkID);
	int desiredRepDeg = chunk.getDesiredReplicationDegree();
	if (chunkRepDeg >= desiredRepDeg) {
		System.out.println("Chunk " + chunkID + " already fulfilled repDegree. Ignoring chunk...");
		return;
	}
}
\end{lstlisting}

Although this method is simple, it is not perfect.
Given there are many concurrent threads running attempting to backup the chunk, there are read and write operation constantly being issued to the hashMap.
Even though the data structure is thread safe, there is no garantee that the number of chunks stored will be exactly the same as the desired replication degree everytime.

There is a possible way to fix this problem, such as resending the protocol until the the exact replication degree is achieved, deleting extra chunks.

Although the aforementioned procedure would present as a solution, we have decided to not implement it, 
as it would flood the multicast channels with several other messages without significantly reducing the space depletion problem.


\subsection{Restore Enhancement}

This enhancment was designed to avoid sending all the file's chunks throung a UDP Multicast channel. As a matter of fact, the 1.0 version is incapable of restoring large files.
given that the UDP protocol is unreliable and there is too much data being continuously sent through the multicast channel, many packets were often missing, and most of them would not even be read,
since only the initiator peer would utilize the body of the message.

In order to solve this problem, we came up with a solution by using the TCP protocol to exchange the body of the message between the CHUNK message and the initiator peer requesting the protocol.

First, we initialized the TCPHandler thread, which is blocked in a while loop, initializing a ClientHandler thread whenever there is a Client Socket initializing the TCP connection.

\begin{lstlisting}[language=java]
public class TCPHandler implements Runnable{

@Override
public void run() {
	while(true) {
		try {
			new ClientHandler(serverSocket.accept()).start();
		} catch (IOException e) {
			e.printStackTrace();
		}
	}
}
...
}
\end{lstlisting}

The ClientHandler thread reads a CHUNK message through the estabilished TCP connection, and restores the file as soon as all the chunks have been received, by launching a GetChunkThread.

The estabilishment of the server side TCP connection is made for each chunk after a Peer that contains its backup initializes a clientSocket in the ChunkThread class.

In order to implement this enhancement, we have changed the header of the GETCHUNK subprotocol by adding an extra line containing the ServerSocket port of the initiator Peer.
This port is essencial to estabilish the TCP connection, as well as the ipAdrress used for the server socket, which is not sent in the message header, given that it can already be accessed
via the UDP Multicast DatagramPacket getAddress() method.

Even though the body of the message is now being sent through a TCP connection to the initiator peer, the multicast channel is still used to send the of the 
CHUNK message. Such is necessary because all peers must update their storage to be able to know if the packet has already been sent by another peer.



\subsection{Delete Enhancement}




	

\end{document}